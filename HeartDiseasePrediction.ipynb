{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dcc33909-7280-424d-abb7-6fae4fce62ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (13,10) and (820,13) not aligned: 10 (dim 1) != 820 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 71\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (A2 \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# 15. Train the model\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m w1, b1, w2, b2 \u001b[38;5;241m=\u001b[39m train_model(X_train, y_train, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# 16. Predict on test data\u001b[39;00m\n\u001b[0;32m     74\u001b[0m y_pred_numeric \u001b[38;5;241m=\u001b[39m predict(X_test, w1, b1, w2, b2)\n",
      "Cell \u001b[1;32mIn[93], line 57\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X, y, hidden_size, output_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     54\u001b[0m w1, b1, w2, b2 \u001b[38;5;241m=\u001b[39m initialize_parameters(input_size, hidden_size, output_size)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):  \u001b[38;5;66;03m# Use range to iterate over epochs\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     Z1, A1, Z2, A2 \u001b[38;5;241m=\u001b[39m forward_propagation(X, w1, b1, w2, b2)\n\u001b[0;32m     58\u001b[0m     loss \u001b[38;5;241m=\u001b[39m binary_cross_entropy(y, A2)\n\u001b[0;32m     59\u001b[0m     dw1, db1, dw2, db2 \u001b[38;5;241m=\u001b[39m backward_propagation(X, y, Z1, A1, A2, w2)\n",
      "Cell \u001b[1;32mIn[93], line 25\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(X, w1, b1, w2, b2)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(X,w1,b1,w2,b2):\n\u001b[1;32m---> 25\u001b[0m     Z1\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdot(w1,X)\u001b[38;5;241m+\u001b[39mb1\n\u001b[0;32m     26\u001b[0m     A1\u001b[38;5;241m=\u001b[39msigmoid(Z1)\n\u001b[0;32m     27\u001b[0m     Z2\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdot(w2,X)\u001b[38;5;241m+\u001b[39mb2\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (13,10) and (820,13) not aligned: 10 (dim 1) != 820 (dim 0)"
     ]
    }
   ],
   "source": [
    "#all necessary imports at the top\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "data=pd.read_csv(\"C://Users//pc//Downloads//heart.csv\")\n",
    "X=data.drop([\"target\"],axis=1).values\n",
    "y=data[\"target\"].values.reshape(-1,1)\n",
    "scaler=StandardScaler()\n",
    "X_scaled=scaler.fit_transform(X)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_scaled,y,test_size=0.2,random_state=42)\n",
    "def initialize_parameters(input_size,hidden_size,output_size):\n",
    "    np.random.seed(42)\n",
    "    w1=np.random.randn(input_size,hidden_size)*0.01\n",
    "    b1=np.zeros((1,hidden_size))\n",
    "    w2=np.random.randn(hidden_size,output_size)*0.01\n",
    "    b2=np.zeros((1,output_size))\n",
    "    return w1,b1,w2,b2\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "def sigmoid_derivative(z):\n",
    "    sigmoid_value = sigmoid(z)\n",
    "    return sigmoid_value * (1 - sigmoid_value)\n",
    "def forward_propagation(X,w1,b1,w2,b2):\n",
    "    Z1=np.dot(w1,X)+b1\n",
    "    A1=sigmoid(Z1)\n",
    "    Z2=np.dot(w2,X)+b2\n",
    "    A2=sigmoid(Z2)\n",
    "    return Z1,A1,Z2,A2\n",
    "def binary_cross_entropy(y, A2):\n",
    "    y = np.array(y, dtype=float)\n",
    "    A2 = np.clip(A2, 1e-7, 1 - 1e-7)\n",
    "    return -np.mean(y * np.log(A2) + (1 - y) * np.log(1 - A2))\n",
    "def backward_propagation(X, y, Z1, A1, A2, w2):\n",
    "    m = X.shape[0]\n",
    "    dZ2 = A2 - y\n",
    "    dw2 = (1 / m) * np.dot(A1.T, dZ2)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "    dA1 = np.dot(dZ2, w2.T)\n",
    "    dZ1 = dA1 * relu_derivative(Z1)\n",
    "    dw1 = (1 / m) * np.dot(X.T, dZ1)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    return dw1, db1, dw2, db2\n",
    "def update_parameters(w1,b1,w2,b2,dw1,db1,dw2,db2,learning_rate):\n",
    "    w1-=learning_rate*dw1\n",
    "    b1-=learning_rate*db1\n",
    "    w2-=learning_rate*dw2\n",
    "    b2-=learning_rate*db2\n",
    "    return w1,b1,w2,b2\n",
    "def train_model(X, y, hidden_size, output_size, epochs, learning_rate):\n",
    "    input_size = X.shape[1]\n",
    "    w1, b1, w2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n",
    "\n",
    "    for epoch in range(epochs):  # Use range to iterate over epochs\n",
    "        Z1, A1, Z2, A2 = forward_propagation(X, w1, b1, w2, b2)\n",
    "        loss = binary_cross_entropy(y, A2)\n",
    "        dw1, db1, dw2, db2 = backward_propagation(X, y, Z1, A1, A2, w2)\n",
    "        w1, b1, w2, b2 = update_parameters(w1, b1, w2, b2, dw1, db1, dw2, db2, learning_rate)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    return w1, b1, w2, b2\n",
    "def predict(X, w1, b1, w2, b2):\n",
    "    _, _, _, A2 = forward_propagation(X, w1, b1, w2, b2)\n",
    "    return (A2 > 0.5).astype(int)\n",
    "\n",
    "# 15. Train the model\n",
    "w1, b1, w2, b2 = train_model(X_train, y_train, hidden_size=10, output_size=1, learning_rate=0.01, epochs=1000)\n",
    "\n",
    "# 16. Predict on test data\n",
    "y_pred_numeric = predict(X_test, w1, b1, w2, b2)\n",
    "\n",
    "# 17. Compare with actual labels directly\n",
    "y_test_numeric = y_test  # No mapping needed; use the original numeric labels\n",
    "\n",
    "# 18. Display some predictions\n",
    "print(\"\\nDisplaying some predictions:\")\n",
    "for i in range(10):  # Display the first 10 predictions as an example\n",
    "    print(f\"Prediction: {y_pred_numeric[i][0]}, Actual: {y_test_numeric[i][0]}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# 19. Evaluate the model\n",
    "accuracy = np.mean(y_pred_numeric == y_test_numeric)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fa6e222-5134-4b46-9cd6-abdc1657f8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
       "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
       "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
       "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
       "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   2     3       0  \n",
       "1   0     3       0  \n",
       "2   0     3       0  \n",
       "3   1     3       0  \n",
       "4   3     2       0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f43c2407-1ae2-4512-82fd-57cbc43f30bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[52.,  1.,  0., ...,  2.,  2.,  3.],\n",
       "       [53.,  1.,  0., ...,  0.,  0.,  3.],\n",
       "       [70.,  1.,  0., ...,  0.,  0.,  3.],\n",
       "       ...,\n",
       "       [47.,  1.,  0., ...,  1.,  1.,  2.],\n",
       "       [50.,  0.,  0., ...,  2.,  0.,  2.],\n",
       "       [54.,  1.,  0., ...,  1.,  1.,  3.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#seperate into input and output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7600116b-1c6d-4d90-8550-c54a6cf3f37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.26843658,  0.66150409, -0.91575542, ...,  0.99543334,\n",
       "         1.20922066,  1.08985168],\n",
       "       [-0.15815703,  0.66150409, -0.91575542, ..., -2.24367514,\n",
       "        -0.73197147,  1.08985168],\n",
       "       [ 1.71659547,  0.66150409, -0.91575542, ..., -2.24367514,\n",
       "        -0.73197147,  1.08985168],\n",
       "       ...,\n",
       "       [-0.81983438,  0.66150409, -0.91575542, ..., -0.6241209 ,\n",
       "         0.23862459, -0.52212231],\n",
       "       [-0.4889957 , -1.51170646, -0.91575542, ...,  0.99543334,\n",
       "        -0.73197147, -0.52212231],\n",
       "       [-0.04787747,  0.66150409, -0.91575542, ..., -0.6241209 ,\n",
       "         0.23862459,  1.08985168]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalization of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a217f473-b399-4bc6-85ae-a27d96371c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data for training and testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d14078ec-af81-406e-a196-3df33cf9c0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8759f85b-cf80-426b-928b-9fa7aff578dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8ffd5a6d-baed-4872-9d58-d9810e07c64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4cc4b0b5-1d36-49fa-9a88-949a88aed221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c50d2734-eed0-4532-af48-adc246113c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "78886d71-3417-4eb8-827e-99deec06ea50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ca9763b5-6ac7-4a05-bbd1-f85e19d4d9ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ef9c7076-a277-479d-a42d-6fb883ca4e5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (13,10) and (820,13) not aligned: 10 (dim 1) != 820 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (A2 \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 15. Train the model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m w1, b1, w2, b2 \u001b[38;5;241m=\u001b[39m train_model(X_train, y_train, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 16. Predict on test data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m y_pred_numeric \u001b[38;5;241m=\u001b[39m predict(X_test, w1, b1, w2, b2)\n",
      "Cell \u001b[1;32mIn[89], line 6\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X, y, hidden_size, output_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m      3\u001b[0m w1, b1, w2, b2 \u001b[38;5;241m=\u001b[39m initialize_parameters(input_size, hidden_size, output_size)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):  \u001b[38;5;66;03m# Use range to iterate over epochs\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     Z1, A1, Z2, A2 \u001b[38;5;241m=\u001b[39m forward_propagation(X, w1, b1, w2, b2)\n\u001b[0;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m binary_cross_entropy(y, A2)\n\u001b[0;32m      8\u001b[0m     dw1, db1, dw2, db2 \u001b[38;5;241m=\u001b[39m backward_propagation(X, y, Z1, A1, A2, w2)\n",
      "Cell \u001b[1;32mIn[61], line 2\u001b[0m, in \u001b[0;36mforward_propagation\u001b[1;34m(X, w1, b1, w2, b2)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(X,w1,b1,w2,b2):\n\u001b[1;32m----> 2\u001b[0m     Z1\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdot(w1,X)\u001b[38;5;241m+\u001b[39mb1\n\u001b[0;32m      3\u001b[0m     A1\u001b[38;5;241m=\u001b[39msigmoid(Z1)\n\u001b[0;32m      4\u001b[0m     Z2\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mdot(w2,X)\u001b[38;5;241m+\u001b[39mb2\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (13,10) and (820,13) not aligned: 10 (dim 1) != 820 (dim 0)"
     ]
    }
   ],
   "source": [
    "# 13. Predict function (numeric labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8008c0e2-1864-4c4c-ac18-37afc42b00a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.6934\n",
      "Epoch 100, Loss: 0.6927\n",
      "Epoch 200, Loss: 0.6926\n",
      "Epoch 300, Loss: 0.6925\n",
      "Epoch 400, Loss: 0.6924\n",
      "Epoch 500, Loss: 0.6923\n",
      "Epoch 600, Loss: 0.6922\n",
      "Epoch 700, Loss: 0.6921\n",
      "Epoch 800, Loss: 0.6919\n",
      "Epoch 900, Loss: 0.6916\n",
      "\n",
      "Displaying some predictions:\n",
      "Prediction: 1, Actual: 1\n",
      "Prediction: 1, Actual: 1\n",
      "Prediction: 1, Actual: 0\n",
      "Prediction: 1, Actual: 1\n",
      "Prediction: 1, Actual: 0\n",
      "Prediction: 1, Actual: 1\n",
      "Prediction: 1, Actual: 0\n",
      "Prediction: 1, Actual: 0\n",
      "Prediction: 1, Actual: 1\n",
      "Prediction: 1, Actual: 0\n",
      "\n",
      "Test accuracy: 0.5024\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"C://Users//pc//Downloads//heart.csv\")\n",
    "\n",
    "# Prepare the data\n",
    "X = data.drop([\"target\"], axis=1).values\n",
    "y = data[\"target\"].values.reshape(-1, 1)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize parameters\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    np.random.seed(42)\n",
    "    w1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "    b1 = np.zeros((1, hidden_size))\n",
    "    w2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "    b2 = np.zeros((1, output_size))\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "# Activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    sigmoid_value = sigmoid(z)\n",
    "    return sigmoid_value * (1 - sigmoid_value)\n",
    "\n",
    "# Forward propagation\n",
    "def forward_propagation(X, w1, b1, w2, b2):\n",
    "    Z1 = np.dot(X, w1) + b1  # Note the order of multiplication\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(A1, w2) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# Loss function\n",
    "def binary_cross_entropy(y, A2):\n",
    "    y = np.array(y, dtype=float)\n",
    "    A2 = np.clip(A2, 1e-7, 1 - 1e-7)\n",
    "    return -np.mean(y * np.log(A2) + (1 - y) * np.log(1 - A2))\n",
    "\n",
    "# Backward propagation\n",
    "def backward_propagation(X, y, Z1, A1, A2, w2):\n",
    "    m = X.shape[0]\n",
    "    dZ2 = A2 - y\n",
    "    dw2 = (1 / m) * np.dot(A1.T, dZ2)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True)\n",
    "\n",
    "    dA1 = np.dot(dZ2, w2.T)\n",
    "    dZ1 = dA1 * sigmoid_derivative(Z1)  # Using sigmoid derivative\n",
    "    dw1 = (1 / m) * np.dot(X.T, dZ1)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True)\n",
    "\n",
    "    return dw1, db1, dw2, db2\n",
    "\n",
    "# Update parameters\n",
    "def update_parameters(w1, b1, w2, b2, dw1, db1, dw2, db2, learning_rate):\n",
    "    w1 -= learning_rate * dw1\n",
    "    b1 -= learning_rate * db1\n",
    "    w2 -= learning_rate * dw2\n",
    "    b2 -= learning_rate * db2\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "# Training the model\n",
    "def train_model(X, y, hidden_size, output_size, epochs, learning_rate):\n",
    "    input_size = X.shape[1]\n",
    "    w1, b1, w2, b2 = initialize_parameters(input_size, hidden_size, output_size)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        Z1, A1, Z2, A2 = forward_propagation(X, w1, b1, w2, b2)\n",
    "        loss = binary_cross_entropy(y, A2)\n",
    "        dw1, db1, dw2, db2 = backward_propagation(X, y, Z1, A1, A2, w2)\n",
    "        w1, b1, w2, b2 = update_parameters(w1, b1, w2, b2, dw1, db1, dw2, db2, learning_rate)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "# Predict function\n",
    "def predict(X, w1, b1, w2, b2):\n",
    "    _, _, _, A2 = forward_propagation(X, w1, b1, w2, b2)\n",
    "    return (A2 > 0.5).astype(int)\n",
    "\n",
    "# Train the model\n",
    "w1, b1, w2, b2 = train_model(X_train, y_train, hidden_size=10, output_size=1, learning_rate=0.01, epochs=1000)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_numeric = predict(X_test, w1, b1, w2, b2)\n",
    "\n",
    "# Compare with actual labels directly\n",
    "y_test_numeric = y_test  # No mapping needed; use the original numeric labels\n",
    "\n",
    "# Display some predictions\n",
    "print(\"\\nDisplaying some predictions:\")\n",
    "for i in range(10):  # Display the first 10 predictions as an example\n",
    "    print(f\"Prediction: {y_pred_numeric[i][0]}, Actual: {y_test_numeric[i][0]}\")\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = np.mean(y_pred_numeric == y_test_numeric)\n",
    "print(f\"\\nTest accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b4d77-ef71-4c16-821e-1d96d539d257",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
